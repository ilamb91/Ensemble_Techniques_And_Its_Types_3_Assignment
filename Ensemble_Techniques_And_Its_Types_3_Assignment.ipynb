{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d324e95b-75c5-48cf-9b43-2c7d5d8fa639",
   "metadata": {},
   "source": [
    "# Q1. What is Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f871c3a-b562-45b3-9c43-adb8a56aa08e",
   "metadata": {},
   "source": [
    "A1\n",
    "\n",
    "A Random Forest Regressor is a machine learning algorithm that belongs to the family of ensemble methods and is used for regression tasks. It is an extension of the Random Forest algorithm, which is primarily used for classification tasks. The Random Forest Regressor, like its classification counterpart, is a powerful and versatile algorithm known for its ability to handle complex regression problems and mitigate overfitting.\n",
    "\n",
    "Here are the key characteristics and components of a Random Forest Regressor:\n",
    "\n",
    "1. **Ensemble of Decision Trees:** The Random Forest Regressor is built upon an ensemble of decision trees. Unlike a single decision tree, which can be prone to overfitting, the ensemble approach aggregates predictions from multiple trees to provide a more robust and accurate regression model.\n",
    "\n",
    "2. **Bootstrap Sampling:** Similar to Random Forest for classification, the Random Forest Regressor employs bootstrap sampling to create multiple subsets (bootstrap samples) of the training data. Each subset is used to train an individual decision tree.\n",
    "\n",
    "3. **Random Feature Subsampling:** During the construction of each decision tree, a random subset of features (input variables) is selected at each split point. This introduces diversity among the trees and helps reduce the correlation between them.\n",
    "\n",
    "4. **Prediction Aggregation:** For regression tasks, the predictions from individual decision trees are typically aggregated by averaging. The final prediction from the Random Forest Regressor is the mean (or weighted mean) of the predictions from all the trees in the ensemble.\n",
    "\n",
    "5. **Reduced Overfitting:** The ensemble of decision trees and the randomness introduced in feature selection and training data sampling work together to reduce overfitting. This results in a more generalizable model that can make accurate predictions on new, unseen data.\n",
    "\n",
    "6. **Hyperparameter Tuning:** Random Forest Regressor has hyperparameters that can be tuned to optimize its performance. Common hyperparameters include the number of trees in the ensemble (n_estimators), the maximum depth of individual trees (max_depth), and the minimum number of samples required to split a node (min_samples_split).\n",
    "\n",
    "7. **Feature Importance:** Random Forest Regressor can provide insights into feature importance. It can rank the input features based on their contribution to the prediction, which is useful for feature selection and understanding the underlying relationships in the data.\n",
    "\n",
    "8. **Applications:** Random Forest Regressor can be applied to a wide range of regression problems, including predictive modeling, time series forecasting, and any task where the goal is to predict a continuous numerical output.\n",
    "\n",
    "The Random Forest Regressor is known for its robustness, versatility, and ability to handle high-dimensional data, making it a popular choice in various domains, such as finance, healthcare, and environmental science. It is particularly effective when dealing with noisy or complex datasets and can provide accurate predictions while maintaining interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47be1af-cd10-42ff-bd8d-6928d263f2f2",
   "metadata": {},
   "source": [
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8a95d-1666-4c91-a197-97f220c63596",
   "metadata": {},
   "source": [
    "A2\n",
    "\n",
    "The Random Forest Regressor reduces the risk of overfitting through a combination of ensemble techniques and randomization strategies that introduce diversity into the model-building process. Here's how Random Forest Regressor achieves this:\n",
    "\n",
    "1. **Ensemble of Decision Trees:** Instead of relying on a single decision tree, the Random Forest Regressor builds an ensemble of decision trees. Each decision tree in the ensemble is trained independently on a bootstrap sample (randomly sampled subset with replacement) of the training data. This ensemble approach helps in reducing the risk of overfitting because individual decision trees may overfit to the noise in the data, but their collective predictions are more robust.\n",
    "\n",
    "2. **Random Feature Subsampling:** When constructing each decision tree in the ensemble, the Random Forest Regressor introduces additional randomness by selecting a random subset of features at each split point. This means that not all features are considered for every split, reducing the likelihood of individual trees fitting to irrelevant or noisy features. The default strategy is often to consider the square root of the total number of features (or a user-defined parameter) as the maximum number of features to consider at each split.\n",
    "\n",
    "3. **Combination of Predictions:** For regression tasks, the predictions from individual decision trees are aggregated to obtain the final prediction. Typically, this aggregation involves taking the mean (or weighted mean) of the individual tree predictions. The aggregation process effectively reduces the variance of the model's predictions, making them more stable and less prone to the idiosyncrasies of individual trees.\n",
    "\n",
    "4. **Pruning and Depth Control:** Although Random Forest Regressor does not perform pruning like traditional decision trees, it still controls the depth of individual trees through hyperparameters like `max_depth`, `min_samples_split`, and `min_samples_leaf`. These hyperparameters limit the depth of the trees, preventing them from growing excessively deep and fitting the training data too closely.\n",
    "\n",
    "5. **Out-of-Bag (OOB) Error Estimation:** Random Forest Regressor can estimate the model's performance on unseen data using the out-of-bag (OOB) error. OOB error is calculated by evaluating each decision tree in the ensemble on the data points that were not included in the bootstrap sample used to train that tree. OOB error provides an estimate of the model's generalization error, helping to monitor and control overfitting.\n",
    "\n",
    "6. **Feature Importance:** Random Forest Regressor can rank the importance of features based on their contribution to the model's predictions. This information can guide feature selection and help identify which features are essential for accurate predictions.\n",
    "\n",
    "By combining the predictions of multiple decision trees, introducing randomness in feature selection and data sampling, and controlling the depth of individual trees, the Random Forest Regressor reduces the risk of overfitting. This ensemble technique makes the model more robust, less sensitive to noise in the training data, and better at generalizing to new, unseen data, ultimately improving its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918bc6d7-560c-42d7-b177-6c7ec38f7835",
   "metadata": {},
   "source": [
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e219cd3-6b8e-4902-8626-19ce4d429c6d",
   "metadata": {},
   "source": [
    "A3.\n",
    "\n",
    "The Random Forest Regressor aggregates the predictions of multiple decision trees using a simple averaging technique. When making predictions for a new data point in a regression task, each decision tree in the Random Forest ensemble generates its own prediction. The final prediction of the Random Forest Regressor is obtained by calculating the mean (or weighted mean) of these individual tree predictions. Here's how the aggregation process works:\n",
    "\n",
    "1. **Individual Tree Predictions:** Each decision tree in the Random Forest Regressor independently predicts the target value (a continuous numerical value) for the given input data point. These individual tree predictions can vary because each tree has been trained on a different bootstrap sample of the training data and has been constructed with randomized feature selection.\n",
    "\n",
    "2. **Prediction Collection:** The predictions from all the individual trees in the ensemble are collected for the same input data point. Each tree contributes its own predicted value.\n",
    "\n",
    "3. **Aggregation:** To obtain the final prediction, the Random Forest Regressor calculates the mean of the individual tree predictions. Specifically, it sums up all the predictions from the trees and divides by the total number of trees. The formula for aggregation is typically:\n",
    "\n",
    "   \\[ \\text{Final Prediction} = \\frac{\\sum_{i=1}^{N} \\text{Tree}_i(\\text{Input Data})}{N} \\]\n",
    "\n",
    "   where:\n",
    "   - \\(N\\) is the number of decision trees in the ensemble.\n",
    "   - \\(\\text{Tree}_i(\\text{Input Data})\\) represents the prediction made by the \\(i\\)-th decision tree for the given input data.\n",
    "\n",
    "4. **Weighted Aggregation (Optional):** In some variations of Random Forest Regressor, weighted aggregation may be used. Instead of giving each tree equal weight in the aggregation process, individual trees may be assigned different weights based on their performance on validation data or other criteria. Weighted aggregation can give more influence to better-performing trees in the ensemble.\n",
    "\n",
    "5. **Final Prediction:** The final prediction, which is the mean of the individual tree predictions or the weighted mean if applicable, represents the estimated continuous target value for the input data point. This final prediction is the output of the Random Forest Regressor for that specific data point.\n",
    "\n",
    "By aggregating the predictions of multiple decision trees, the Random Forest Regressor leverages the wisdom of the crowd, reducing the variance of the model and improving its overall predictive accuracy. This ensemble approach helps ensure that the model's predictions are more stable and reliable than those of any individual tree, making it a powerful tool for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d35de-69f9-4df1-af17-6958515939bd",
   "metadata": {},
   "source": [
    "# Q4. What are the hyperparameters of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f1104-c774-419a-a6b1-81296ee9267e",
   "metadata": {},
   "source": [
    "A4\n",
    "\n",
    "The Random Forest Regressor in scikit-learn and other machine learning libraries offers several hyperparameters that allow you to fine-tune and customize the behavior of the algorithm. Here are some of the most commonly used hyperparameters for the Random Forest Regressor:\n",
    "\n",
    "1. **n_estimators:** This hyperparameter specifies the number of decision trees in the ensemble. A larger number of trees generally leads to a more robust model, but it can also increase computational complexity. You can set it to control the ensemble size.\n",
    "\n",
    "2. **criterion:** It defines the function used to measure the quality of a split. For regression tasks, \"mse\" (mean squared error) is commonly used, but you can also choose \"mae\" (mean absolute error) as an alternative.\n",
    "\n",
    "3. **max_depth:** This parameter limits the maximum depth of individual decision trees in the ensemble. It can help control the complexity of the trees and prevent overfitting. Setting it to None allows trees to expand until they contain fewer than min_samples_split samples.\n",
    "\n",
    "4. **min_samples_split:** It sets the minimum number of samples required to split an internal node. Increasing this value can help control tree growth and prevent overfitting.\n",
    "\n",
    "5. **min_samples_leaf:** This parameter specifies the minimum number of samples required to be a leaf node. It helps control the size of the leaves and can also prevent overfitting.\n",
    "\n",
    "6. **max_features:** It determines the maximum number of features to consider when looking for the best split at each node. You can set it as an integer (e.g., \"max_features=5\") or as a fraction of the total number of features (e.g., \"max_features='sqrt'\").\n",
    "\n",
    "7. **bootstrap:** If set to True (the default), it indicates whether to use bootstrap sampling when building each decision tree. Bootstrap sampling involves sampling with replacement from the training data, introducing randomness into the dataset.\n",
    "\n",
    "8. **random_state:** This seed value controls the randomness of the algorithm. Setting it to a specific integer ensures reproducibility of results.\n",
    "\n",
    "9. **n_jobs:** It specifies the number of CPU cores to use for parallel processing during training. Setting it to -1 uses all available cores.\n",
    "\n",
    "10. **oob_score:** When set to True, it enables out-of-bag (OOB) scoring. OOB error estimates the model's performance on unseen data points that were not used in the bootstrap samples.\n",
    "\n",
    "11. **verbose:** This hyperparameter controls the level of verbosity of the algorithm's output during training. Increasing the verbosity level provides more information about the training process.\n",
    "\n",
    "12. **warm_start:** If set to True, it allows you to add more trees to an existing Random Forest (incremental training). Useful when you want to train the model in stages.\n",
    "\n",
    "These hyperparameters offer flexibility in tuning the Random Forest Regressor for specific regression tasks. The optimal values for these hyperparameters may vary depending on the dataset and the problem you are trying to solve, so hyperparameter tuning, often using techniques like grid search or random search, can be essential to finding the best configuration for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8430c-337b-4a0a-8f10-cec5df7a82e0",
   "metadata": {},
   "source": [
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5615620-947e-4f0f-b415-f12219a819d6",
   "metadata": {},
   "source": [
    "A5\n",
    "\n",
    "The Random Forest Regressor and the Decision Tree Regressor are both machine learning algorithms used for regression tasks, but they differ in several key aspects:\n",
    "\n",
    "1. **Ensemble vs. Single Model:**\n",
    "\n",
    "   - **Random Forest Regressor:** It is an ensemble learning method that combines multiple decision trees to make predictions. The predictions from individual trees are aggregated (usually by averaging) to obtain the final prediction. This ensemble approach reduces overfitting and improves prediction accuracy.\n",
    "\n",
    "   - **Decision Tree Regressor:** It is a single decision tree model used for regression tasks. A decision tree recursively splits the data into subsets based on feature values and creates a tree structure to make predictions. Decision trees can be prone to overfitting when they become too deep or complex.\n",
    "\n",
    "2. **Handling Overfitting:**\n",
    "\n",
    "   - **Random Forest Regressor:** It reduces the risk of overfitting by combining predictions from multiple decision trees, each trained on a different subset of the data. The ensemble approach introduces diversity, mitigating the overfitting problem seen in individual decision trees.\n",
    "\n",
    "   - **Decision Tree Regressor:** A single decision tree can easily overfit the training data if it is allowed to grow too deep. While decision tree regressors offer options like controlling the maximum depth and minimum samples per leaf, they may require additional pruning or regularization techniques to avoid overfitting.\n",
    "\n",
    "3. **Predictive Accuracy:**\n",
    "\n",
    "   - **Random Forest Regressor:** It generally provides higher predictive accuracy compared to a single decision tree. The ensemble of decision trees, each offering its own perspective, tends to produce more robust and accurate predictions.\n",
    "\n",
    "   - **Decision Tree Regressor:** While decision tree regressors can perform well on simple datasets, they may not generalize as effectively on more complex or noisy data due to their tendency to overfit.\n",
    "\n",
    "4. **Interpretability:**\n",
    "\n",
    "   - **Random Forest Regressor:** The ensemble of decision trees is less interpretable compared to a single decision tree. Understanding the decision-making process of a Random Forest can be challenging because it involves aggregating predictions from multiple trees.\n",
    "\n",
    "   - **Decision Tree Regressor:** A single decision tree is highly interpretable and can be visualized to reveal the decision-making logic. This interpretability is an advantage when transparency and understanding of the model are crucial.\n",
    "\n",
    "5. **Use Cases:**\n",
    "\n",
    "   - **Random Forest Regressor:** It is often preferred when predictive accuracy is a top priority, especially in complex regression tasks with large datasets or high-dimensional feature spaces. Common applications include finance, healthcare, and environmental modeling.\n",
    "\n",
    "   - **Decision Tree Regressor:** It can be suitable for simpler regression problems where interpretability is essential, and you want a clear understanding of the model's decision process. Decision tree regressors can serve as the building blocks of more complex models as well.\n",
    "\n",
    "In summary, the primary difference between the Random Forest Regressor and the Decision Tree Regressor lies in their approach to handling overfitting and predictive accuracy. Random Forest Regressor is an ensemble method that leverages multiple decision trees to reduce overfitting and improve accuracy, making it well-suited for complex regression tasks. Decision Tree Regressor is a single-model approach that is highly interpretable but may be more susceptible to overfitting on challenging datasets. The choice between the two depends on the specific requirements of your regression problem and your priorities regarding accuracy and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e897e-3f32-437e-94ce-47a0122b25f7",
   "metadata": {},
   "source": [
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5279c96-4d56-44a1-bbcd-294f8103bc9f",
   "metadata": {},
   "source": [
    "A6\n",
    "\n",
    "The Random Forest Regressor is a powerful machine learning algorithm with several advantages and some potential disadvantages. Understanding these pros and cons can help you decide when to use it and how to mitigate its limitations. Here are the advantages and disadvantages of the Random Forest Regressor:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **High Predictive Accuracy:** Random Forest Regressor is known for its excellent predictive performance. It often outperforms single decision tree models and other regression algorithms, making it a top choice when accuracy is a priority.\n",
    "\n",
    "2. **Reduced Overfitting:** The ensemble nature of the Random Forest reduces the risk of overfitting, even on complex and noisy datasets. By aggregating the predictions of multiple decision trees, it creates a more robust and generalized model.\n",
    "\n",
    "3. **Handling of High-Dimensional Data:** Random Forests can effectively handle datasets with a large number of features (high dimensionality) without the need for feature selection. The random feature subsetting helps in reducing the impact of irrelevant features.\n",
    "\n",
    "4. **Non-linearity and Complex Relationships:** Random Forest Regressor can capture complex non-linear relationships in the data, making it suitable for a wide range of regression tasks, including those with intricate interactions between variables.\n",
    "\n",
    "5. **Feature Importance:** The algorithm can rank the importance of features, allowing you to identify which variables have the most significant impact on predictions. This can guide feature selection and data analysis.\n",
    "\n",
    "6. **Robustness to Outliers:** Random Forests are less sensitive to outliers and noisy data points compared to some other regression techniques. The aggregation of multiple tree predictions helps mitigate the effect of outliers.\n",
    "\n",
    "7. **Out-of-Bag (OOB) Error Estimation:** Random Forests can provide an estimate of the model's performance on unseen data using OOB error. This built-in validation measure allows you to assess the model's generalization capability without requiring a separate validation set.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Less Interpretability:** Random Forest Regressor, as an ensemble of decision trees, is less interpretable than a single decision tree. Understanding the rationale behind individual predictions can be challenging.\n",
    "\n",
    "2. **Computationally Intensive:** Training and predicting with Random Forests can be computationally expensive, especially with a large number of trees or features. This can be a drawback when working with limited computational resources.\n",
    "\n",
    "3. **Hyperparameter Tuning:** To optimize performance, Random Forests may require tuning of hyperparameters, such as the number of trees, maximum depth, and feature selection criteria. This tuning process can be time-consuming.\n",
    "\n",
    "4. **Overhead:** The creation and maintenance of an ensemble with multiple decision trees come with some computational overhead. This may affect real-time or resource-constrained applications.\n",
    "\n",
    "5. **Not Suitable for All Data:** While Random Forests perform well on a wide range of tasks, they may not be the best choice for datasets with a low signal-to-noise ratio or when a simpler model can provide sufficient accuracy.\n",
    "\n",
    "6. **Bias Toward Dominant Classes (Classification):** In classification tasks, Random Forests may exhibit a slight bias toward dominant classes in imbalanced datasets. Careful handling may be required in such cases.\n",
    "\n",
    "In summary, the Random Forest Regressor offers substantial advantages in terms of predictive accuracy, robustness to overfitting, and the ability to handle complex data. However, it comes with the trade-off of reduced interpretability and potentially higher computational requirements. Understanding these trade-offs and selecting appropriate hyperparameters can help you make the most of its strengths while mitigating its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b66b8-be5a-4b85-83f7-9f523e2ad407",
   "metadata": {},
   "source": [
    "# Q7. What is the output of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66792dae-970a-407b-800d-7b2c2657247f",
   "metadata": {},
   "source": [
    "A7\n",
    "\n",
    "The output of a Random Forest Regressor is a prediction of a continuous numerical value for a given input or set of inputs. In other words, it provides an estimate of the target variable (the variable you want to predict) as a real number. This prediction represents the model's best guess of the numerical outcome based on the input features and the patterns it has learned from the training data.\n",
    "\n",
    "Here's how the output of a Random Forest Regressor is typically used:\n",
    "\n",
    "1. **Single Prediction:** For a single input data point (a set of feature values), the Random Forest Regressor produces a single predicted numerical value. This prediction is an estimate of the target variable for that specific data point.\n",
    "\n",
    "2. **Multiple Predictions:** If you have multiple data points (e.g., a batch of test data), you can use the Random Forest Regressor to make predictions for all of them simultaneously. Each input data point is processed, and the model generates a predicted value for each one.\n",
    "\n",
    "3. **Continuous Range:** The predicted values can cover a continuous range of numerical values, depending on the nature of the regression problem. For example, if you're predicting house prices, the predictions might be in the range of tens of thousands to millions of dollars.\n",
    "\n",
    "4. **Evaluation:** You can compare the model's predictions to the actual target values in your test or validation dataset to evaluate its performance. Common evaluation metrics for regression tasks include mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared (coefficient of determination).\n",
    "\n",
    "5. **Decision-Making:** The predicted values can be used for decision-making, such as estimating sales prices, forecasting demand, or predicting future trends based on historical data.\n",
    "\n",
    "The Random Forest Regressor aggregates the predictions of multiple decision trees in the ensemble to provide a more accurate and robust prediction. The ensemble nature of the model allows it to capture complex relationships in the data while reducing the risk of overfitting. The output of the Random Forest Regressor serves as a valuable tool for making numerical predictions in various regression tasks, ranging from finance to healthcare to environmental modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188583ed-bf6e-4203-86bc-66ac8b501f56",
   "metadata": {},
   "source": [
    "# Q8. Can Random Forest Regressor be used for classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0feb22-f230-4f33-9603-058908d66716",
   "metadata": {},
   "source": [
    "A8.\n",
    "\n",
    "The primary purpose of the Random Forest Regressor is to perform regression tasks, where the goal is to predict continuous numerical values. However, the Random Forest algorithm has a counterpart designed specifically for classification tasks, called the Random Forest Classifier. \n",
    "\n",
    "In a Random Forest Classifier, the algorithm is configured to handle classification problems, where the objective is to assign data points to predefined categories or classes. It does this by utilizing an ensemble of decision trees, just like the Random Forest Regressor, but the predictions are for discrete class labels rather than continuous numerical values.\n",
    "\n",
    "Here are the key differences:\n",
    "\n",
    "- **Output:** \n",
    "  - Random Forest Regressor: Predicts continuous numerical values (e.g., predicting house prices, stock prices).\n",
    "  - Random Forest Classifier: Predicts discrete class labels (e.g., classifying emails as spam or not spam, classifying images of animals into categories like \"cat,\" \"dog,\" or \"horse\").\n",
    "\n",
    "- **Objective:** \n",
    "  - Random Forest Regressor: Minimizes the mean squared error (MSE) or other regression-oriented loss function.\n",
    "  - Random Forest Classifier: Maximizes classification accuracy or minimizes classification error.\n",
    "\n",
    "- **Training Data:** \n",
    "  - Random Forest Regressor: Uses training data with continuous target variables (e.g., house prices).\n",
    "  - Random Forest Classifier: Uses training data with categorical or discrete target labels (e.g., class labels like \"spam\" or \"ham\").\n",
    "\n",
    "- **Evaluation Metrics:** \n",
    "  - Random Forest Regressor: Evaluated using regression metrics such as MSE, RMSE, MAE, and R-squared.\n",
    "  - Random Forest Classifier: Evaluated using classification metrics like accuracy, precision, recall, F1-score, and ROC AUC.\n",
    "\n",
    "If your task involves classifying data into distinct categories or classes, you should use the Random Forest Classifier or another classification algorithm that is designed for this specific purpose. The Random Forest Regressor is not intended for classification tasks, and using it for classification would require adapting the algorithm and the target variable, which may not yield meaningful or accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99932a16-0d9c-4df7-96cd-46d70fe7f516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
